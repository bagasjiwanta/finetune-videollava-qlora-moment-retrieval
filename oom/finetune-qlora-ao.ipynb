{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f762d11-5dc4-47cf-9de4-ddb51f54b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.dataset.collate import DataCollatorWithPadding\n",
    "from project.dataset.prepare import DatasetPreparer\n",
    "from project.trainer.peft import find_all_linear_names\n",
    "\n",
    "from transformers import (\n",
    "    VideoLlavaProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    VideoLlavaForConditionalGeneration\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29ff643-44cc-497d-a33f-a0e5e2d2e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import LightningModule\n",
    "from project.trainer.metrics import ao_exact_score\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "\n",
    "\n",
    "class VideoLlavaModelPLModule(LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch):\n",
    "\n",
    "        input_ids, attention_mask, pixel_values_videos, labels = batch\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values_videos=pixel_values_videos,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        print(loss)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "\n",
    "        input_ids, attention_mask, pixel_values_videos, labels = batch\n",
    "        is_ao = len(labels[0][0]) == 1 \n",
    "\n",
    "        if not is_ao:\n",
    "            frame_info = batch[-1]\n",
    "\n",
    "        # autoregressively generate token IDs\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values_videos=pixel_values_videos,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        predictions = self.processor.batch_decode(\n",
    "            generated_ids[:, input_ids.size(1):], \n",
    "            skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        if is_ao:\n",
    "            score, correct = ao_exact_score(predictions, labels)\n",
    "        else:\n",
    "            score = 1\n",
    "            correct = len(predictions)\n",
    "        self.log(\"val_accuracy\", score)\n",
    "\n",
    "        return correct\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # use 8 bit optimizer\n",
    "        optimizer = Adam8bit(self.parameters(), min_8bit_size=4096, lr=self.config.get(\"lr\"))\n",
    "        # optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa4ca30-e640-4cee-b3a4-0371c46345da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    lora_r: int = 4\n",
    "    lora_alpha: int = 8\n",
    "    batch_size: int = 1\n",
    "    max_epoch: int = 2\n",
    "    val_check_interval: float = 0.25\n",
    "    learning_rate: float = 2e-5\n",
    "    dataset_dir: str = \"datasets/processed\"\n",
    "    num_frames: int = 14\n",
    "    num_worker: int = 2\n",
    "    hub_repo: str = \"jwnt4/finetune-videollava-qlora\"\n",
    "    accumulate_grad_batches: int = 1\n",
    "    limit_val_batches: float = 16\n",
    "\n",
    "args = Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f206f1db-f876-49a4-a185-87f187919bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6f75fd-f784-4ce4-803c-09ec0d8fa0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.makedirs(\"logs\")\n",
    "\n",
    "log_file = f\"logs/{str(datetime.now()).replace(' ', '_')}.log\"\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "\n",
    "log_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\")\n",
    "stream_handler.setFormatter(log_formatter)\n",
    "\n",
    "logger.addHandler(stream_handler)\n",
    "logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6894bb14-fecf-4e7c-8741-3e58f3da07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", use_fast=False)\n",
    "processor.patch_size = 14\n",
    "processor.vision_feature_select_strategy = \"default\"\n",
    "processor.tokenizer.deprecation_warnings[\"Asking-to-pad-a-fast-tokenizer\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "785bf6fc-0248-4acf-9bae-15dc455a58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = args.dataset_dir.split(\"/\")[0]\n",
    "processed_dir = args.dataset_dir.split(\"/\")[1]\n",
    "dp = DatasetPreparer(\n",
    "    base_dir=base_dir, processed_dir=processed_dir, num_frames=args.num_frames, num_worker=2, processor=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb4d619f-c985-48dc-a7b9-2047dcf041bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None\n",
    "try:\n",
    "    dataset = load_from_disk(f\"{args.dataset_dir}/action_ordering_v2/robust/{args.num_frames}_frames\")\n",
    "except:\n",
    "    dataset = None\n",
    "if dataset is None:\n",
    "    dataset = dp.prepare_dataset('action_ordering_v2', use_robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8af930d-e863-483b-8215-e06d983f30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_dataloader = DataLoader(dataset['train'], collate_fn=DataCollatorWithPadding(processor), batch_size=args.batch_size, shuffle=False, num_workers=2)\n",
    "    eval_dataloader = DataLoader(dataset['validation'], collate_fn=DataCollatorWithPadding(processor), batch_size=args.batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e85a60-ec27-4fe0-afe6-d09a7be63a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1404d9bce32a4740ac1704cd5ce68866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define quantized model in \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "    \"LanguageBind/Video-LLaVA-7B-hf\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.generation_config.max_new_tokens = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1b98937-65d5-4149-b487-c99ac15cb237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): VideoLlavaForConditionalGeneration(\n",
       "      (video_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(257, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPSdpaAttention(\n",
       "                  (k_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (fc2): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (image_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(257, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPSdpaAttention(\n",
       "                  (k_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (fc2): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): VideoLlavaMultiModalProjector(\n",
       "        (linear_1): lora.Linear4bit(\n",
       "          (base_layer): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=1024, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (act): GELUActivation()\n",
       "        (linear_2): lora.Linear4bit(\n",
       "          (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (language_model): LlamaForCausalLM(\n",
       "        (model): LlamaModel(\n",
       "          (embed_tokens): Embedding(32064, 4096, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaSdpaAttention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=11008, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=11008, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=11008, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=args.lora_r,\n",
    "    lora_alpha=args.lora_alpha,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    init_lora_weights=\"gaussian\"\n",
    ") \n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b28541c-1525-442c-9b31-d75907230ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoLlavaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"LanguageBind/Video-LLaVA-7B-hf\",\n",
      "  \"architectures\": [\n",
      "    \"VideoLlavaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_seq_length\": 256,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"video_llava\",\n",
      "  \"pad_token_id\": 32002,\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"pad_token_id\": 0,\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"video_seq_length\": 2056,\n",
      "  \"video_token_index\": 32001,\n",
      "  \"vision_config\": {\n",
      "    \"hidden_size\": 1024,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"patch_size\": 14,\n",
      "    \"projection_dim\": 768,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\",\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 40,\n",
      "  \"pad_token_id\": 32002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)\n",
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05696db2-7560-4dfa-8d91-09e41348e11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = VideoLlavaModelPLModule(\n",
    "    config={\n",
    "        \"lr\": args.learning_rate\n",
    "    },\n",
    "    processor=processor,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_accuracy\", verbose=False, mode=\"min\")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    monitor='val_accuracy',\n",
    "    dirpath='output/',\n",
    "    filename='videollava-7b-ao-{epoch:02d}-{val_accuracy:.2f}'+f\"lora_r{args.lora_r}-lora_alpha{args.lora_alpha}\"\n",
    ")\n",
    "callbacks = [\n",
    "    early_stopping, model_checkpoint\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd457cc-c004-4391-b135-cd68bc7a029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_val_batches = (args.limit_val_batches // args.batch_size) * args.batch_size\n",
    "train_conf = {\n",
    "    \"max_epochs\": args.max_epoch,\n",
    "    \"accumulate_grad_batches\": args.accumulate_grad_batches,\n",
    "    \"limit_val_batches\": int(limit_val_batches),\n",
    "    \"val_check_interval\": args.val_check_interval,\n",
    "    \"precision\": \"16-mixed\",\n",
    "    \"gradient_clip_val\": 1.0,\n",
    "    \"num_sanity_val_steps\": int(args.batch_size * 8)\n",
    "}\n",
    "logger.info(str(train_conf))\n",
    "\n",
    "trainer = Trainer(\n",
    "    **train_conf,\n",
    "    accelerator=\"auto\",\n",
    "    devices=[0],\n",
    "    callbacks=callbacks,\n",
    "    strategy=\"deepspeed_stage_2_offload\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed64fbd7-291c-42f7-955e-fdb948ebb21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-14 19:51:06,260] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:05<00:00,  1.78s/it]\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n",
      "12/14/2024 19:51:19 - INFO - {'max_epochs': 2, 'accumulate_grad_batches': 1, 'limit_val_batches': 16, 'val_check_interval': 0.25, 'precision': '16-mixed', 'gradient_clip_val': 1.0, 'num_sanity_val_steps': 8}\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:44: Attribute 'model' removed from hparams because it cannot be pickled. You can suppress this warning by setting `self.save_hyperparameters(ignore=['model'])`.\n",
      "initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Enabling DeepSpeed FP16. Model parameters and inputs will be cast to `float16`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "You have specified an optimizer and/or scheduler within the DeepSpeed config. It is recommended to define it in `LightningModule.configure_optimizers`.\n",
      "[2024-12-14 19:51:24,531] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "Using /root/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py311_cu124/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.4477946758270264 seconds\n",
      "[2024-12-14 19:51:37,146] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | PeftModel | 3.8 B  | train\n",
      "--------------------------------------------\n",
      "13.6 M    Trainable params\n",
      "3.8 B     Non-trainable params\n",
      "3.8 B     Total params\n",
      "15,317.549Total estimated model params size (MB)\n",
      "5142      Modules in train mode\n",
      "1054      Modules in eval mode\n",
      "/opt/conda/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'model' parameter because it is not possible to safely dump to YAML.\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/8 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 8/8 [00:08<00:00,  0.96it/s]/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_accuracy', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "Epoch 0:   0%|                                          | 0/596 [00:00<?, ?it/s][rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/home/finetune-videollava-lora/deepspeed-finetune-qlora-ao.py\", line 177, in <module>\n",
      "[rank0]:     trainer.fit(module, train_dataloader, eval_dataloader)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 538, in fit\n",
      "[rank0]:     call._call_and_handle_interrupt(\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
      "[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "[rank0]:     return function(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 574, in _fit_impl\n",
      "[rank0]:     self._run(model, ckpt_path=ckpt_path)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 981, in _run\n",
      "[rank0]:     results = self._run_stage()\n",
      "[rank0]:               ^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py\", line 1025, in _run_stage\n",
      "[rank0]:     self.fit_loop.run()\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 205, in run\n",
      "[rank0]:     self.advance()\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py\", line 363, in advance\n",
      "[rank0]:     self.epoch_loop.run(self._data_fetcher)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 140, in run\n",
      "[rank0]:     self.advance(data_fetcher)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 250, in advance\n",
      "[rank0]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 190, in run\n",
      "[rank0]:     self._optimizer_step(batch_idx, closure)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 268, in _optimizer_step\n",
      "[rank0]:     call._call_lightning_module_hook(\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 167, in _call_lightning_module_hook\n",
      "[rank0]:     output = fn(*args, **kwargs)\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/core/module.py\", line 1306, in optimizer_step\n",
      "[rank0]:     optimizer.step(closure=optimizer_closure)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py\", line 153, in step\n",
      "[rank0]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/strategies/ddp.py\", line 270, in optimizer_step\n",
      "[rank0]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)\n",
      "[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 238, in optimizer_step\n",
      "[rank0]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/deepspeed.py\", line 129, in optimizer_step\n",
      "[rank0]:     closure_result = closure()\n",
      "[rank0]:                      ^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 144, in __call__\n",
      "[rank0]:     self._result = self.closure(*args, **kwargs)\n",
      "[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "[rank0]:     return func(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 129, in closure\n",
      "[rank0]:     step_output = self._step_fn()\n",
      "[rank0]:                   ^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py\", line 317, in _training_step\n",
      "[rank0]:     training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py\", line 319, in _call_strategy_hook\n",
      "[rank0]:     output = fn(*args, **kwargs)\n",
      "[rank0]:              ^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 389, in training_step\n",
      "[rank0]:     return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 640, in __call__\n",
      "[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)\n",
      "[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py\", line 18, in wrapped_fn\n",
      "[rank0]:     ret_val = func(*args, **kwargs)\n",
      "[rank0]:               ^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py\", line 1909, in forward\n",
      "[rank0]:     loss = self.module(*inputs, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py\", line 633, in wrapped_forward\n",
      "[rank0]:     out = method(*_args, **_kwargs)\n",
      "[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/finetune-videollava-lora/project/trainer/lightning.py\", line 18, in training_step\n",
      "[rank0]:     outputs = self.model(\n",
      "[rank0]:               ^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/peft/peft_model.py\", line 849, in forward\n",
      "[rank0]:     return self.get_base_model()(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/transformers/models/video_llava/modeling_video_llava.py\", line 663, in forward\n",
      "[rank0]:     outputs = self.language_model(\n",
      "[rank0]:               ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 1163, in forward\n",
      "[rank0]:     outputs = self.model(\n",
      "[rank0]:               ^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 913, in forward\n",
      "[rank0]:     layer_outputs = decoder_layer(\n",
      "[rank0]:                     ^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 655, in forward\n",
      "[rank0]:     hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "[rank0]:     return forward_call(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 73, in forward\n",
      "[rank0]:     variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "[rank0]:                ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.43 GiB of which 38.06 MiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 629.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[rank0]:[W1214 19:51:54.795791898 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n"
     ]
    }
   ],
   "source": [
    "!python deepspeed-finetune-qlora-ao.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f82bc5-5257-4950-87b0-5428f9ebb31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
